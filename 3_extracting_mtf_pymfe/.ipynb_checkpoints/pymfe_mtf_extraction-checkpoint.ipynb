{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompEngine dataset analysis\n",
    "## Analysis #3: Explore balanced subset\n",
    "\n",
    "**Project URL:** https://www.comp-engine.org/\n",
    "\n",
    "**Get data in:** https://www.comp-engine.org/#!browse\n",
    "\n",
    "**Date:** May 29 2020\n",
    "\n",
    "### Objectives:\n",
    "1. Extract the meta-features using the unsupervised methods in pymfe from train and test data\n",
    "2. Drop metafeatures with NaN (since we can't apply PCA in those)..\n",
    "3. Apply PCA in the train set meta-features.\n",
    "4. Use a simple machine learning model to predict the test set.\n",
    "\n",
    "### Results (please check the analysis date):\n",
    "1. All metafeatures from all unsupervised methods combined with all summary functions in pymfe were extracted from both train and test data. This totalizes 1407 candidate meta-features.\n",
    "    1. Before extraction, every time-series were embedded in the appropriate lag (using the first non-significative lag of the autocorrelation function) and appropriate dimension (using Cao's algorithm).\n",
    "    2. The minimum embedding dimension was set to 2 to avoid losing too many meta-features in the summarization process.\n",
    "    3. Only up to 1024 most recent observations of each time-series were used.\n",
    "2. We want to apply PCA in the data, but first we need to get rid of the missing data in the training meta-data. There is a total of 121 or 8.60% of meta-features with at least one missing value. After dropping all meta-features with missing values, 1286 remains.\n",
    "    1. There were 112 meta-features with 310 missing values (missing on 42.12% of all train time-series);\n",
    "    2. There were 8 meta-features with 584 missing values (missing on 79.35% of all train time-series);\n",
    "    3. There was 1 meta-feature (\"num_to_cat\") with 736 missing values (missing on all train time-series). As a side node, this result seems very reasonable.\n",
    "3. The next step is to apply PCA retaining 95% of variance explained by the original meta-features. Before applying PCA we need to choose a normalization strategy. Two methods were considered:\n",
    "    1. (Pipeline A) Standard Scaler (traditional standardization): 105 of 1286 dimensions were kept. This corresponds to a dimension reduction of 91.84%.\n",
    "    2. (Pipeline B) Robust Sigmoid Scaler (see reference [1]): 63 of 1286 dimensions were kept. This corresponds to a dimension reduction of 95.10%.\n",
    "4. Now it is time for some predictions. I'm using a sklearn RandomForestClassifier model with default hyper-parameters with a fixed random seed.\n",
    "    1. The expected accuracy of random guessing is 2.17%.\n",
    "    2. (Pipeline A) It was obtained an accuracy score of 47.28%.\n",
    "    3. (Pipeline B) It was obtained an accuracy score of 55.43%.\n",
    "    \n",
    "\n",
    "## references:\n",
    "\n",
    ".. [1] Fulcher, Ben D.  and Little, Max A.  and Jones, Nick S., \"Highly comparative time-series analysis: the empirical structure of time series and their methods\" (Supplemental material #1, page 11), Journal of The Royal Society Interface, 2013, doi: 10.1098/rsif.2013.0048, https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2013.0048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "\n",
    "import robust_sigmoid\n",
    "import pymfe.mfe\n",
    "import tspymfe._embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Warning: This Pymfe source code is to guarantee reproducibility in the experimental tests of the proposed pymfe expansion for time-series.\n",
      "   Please use only the updated version available at: https://github.com/ealcobaca/pymfe\n"
     ]
    }
   ],
   "source": [
    "# Note: using only groups that has at least one meta-feature that can be extracted\n",
    "# from a unsupervised dataset\n",
    "groups = (\"general\", \"statistical\", \"info-theory\", \"complexity\", \"itemset\", \"concept\")\n",
    "summary = \"all\"\n",
    "\n",
    "extractor = pymfe.mfe.MFE(features=\"all\",\n",
    "                          summary=summary,\n",
    "                          groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../2_exploring_subsample/subsample_train.csv\", header=0, index_col=\"timeseries_id\")\n",
    "data_test = pd.read_csv(\"../2_exploring_subsample/subsample_test.csv\", header=0, index_col=\"timeseries_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>inst_ind</th>\n",
       "      <th>datapoints</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeseries_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e0b36e39-3872-11e8-8680-0242ac120002</th>\n",
       "      <td>Beta noise</td>\n",
       "      <td>25254</td>\n",
       "      <td>0.73617,0.99008,0.71331,0.87094,0.75527,0.9912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81db0cf2-3883-11e8-8680-0242ac120002</th>\n",
       "      <td>Relative humidity</td>\n",
       "      <td>14878</td>\n",
       "      <td>95.5,79,86.75,8.75,62.75,98.75,79.74,44.75,92....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380eb353-387a-11e8-8680-0242ac120002</th>\n",
       "      <td>RR</td>\n",
       "      <td>6577</td>\n",
       "      <td>0.6328,0.6328,0.625,0.6328,0.625,0.625,0.6172,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f33f461c-3871-11e8-8680-0242ac120002</th>\n",
       "      <td>Tremor</td>\n",
       "      <td>27821</td>\n",
       "      <td>-0.6,1.5,1.5,0.1,0.9,0.6,0.3,-0.2,0.7,1,0.1,1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7bcad309-3874-11e8-8680-0242ac120002</th>\n",
       "      <td>Noisy sinusoids</td>\n",
       "      <td>14226</td>\n",
       "      <td>0.38553,0.2014,1.8705,0.47883,0.33958,0.009558...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               category  inst_ind  \\\n",
       "timeseries_id                                                       \n",
       "e0b36e39-3872-11e8-8680-0242ac120002         Beta noise     25254   \n",
       "81db0cf2-3883-11e8-8680-0242ac120002  Relative humidity     14878   \n",
       "380eb353-387a-11e8-8680-0242ac120002                 RR      6577   \n",
       "f33f461c-3871-11e8-8680-0242ac120002             Tremor     27821   \n",
       "7bcad309-3874-11e8-8680-0242ac120002    Noisy sinusoids     14226   \n",
       "\n",
       "                                                                             datapoints  \n",
       "timeseries_id                                                                            \n",
       "e0b36e39-3872-11e8-8680-0242ac120002  0.73617,0.99008,0.71331,0.87094,0.75527,0.9912...  \n",
       "81db0cf2-3883-11e8-8680-0242ac120002  95.5,79,86.75,8.75,62.75,98.75,79.74,44.75,92....  \n",
       "380eb353-387a-11e8-8680-0242ac120002  0.6328,0.6328,0.625,0.6328,0.625,0.625,0.6172,...  \n",
       "f33f461c-3871-11e8-8680-0242ac120002  -0.6,1.5,1.5,0.1,0.9,0.6,0.3,-0.2,0.7,1,0.1,1....  \n",
       "7bcad309-3874-11e8-8680-0242ac120002  0.38553,0.2014,1.8705,0.47883,0.33958,0.009558...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert data_train.shape[0] > data_test.shape[0]\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: using at most the last 1024 observations of each time-series\n",
    "size_threshold = 1024\n",
    "\n",
    "# Number of iterations until to save results to .csv\n",
    "to_csv_it_num = 16\n",
    "\n",
    "# Note: using dummy data to get the metafeature names\n",
    "mtf_names = extractor.fit(np.arange(16).reshape(-1, 2),\n",
    "                          suppress_warnings=True).extract(suppress_warnings=True)[0]\n",
    "\n",
    "# Note: filepath to store the results\n",
    "filename_train = \"metafeatures_pymfe_train.csv\"\n",
    "filename_test = \"metafeatures_pymfe_test.csv\"\n",
    "\n",
    "def recover_data(filepath: str,\n",
    "                 index: typing.Collection[str],\n",
    "                 def_shape: typing.Tuple[int, int]) -> typing.Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"Recover data from the previous experiment run.\"\"\"\n",
    "    filled_len = 0\n",
    "    \n",
    "    try:\n",
    "        results = pd.read_csv(filepath, index_col=0)\n",
    "        \n",
    "        assert results.shape == def_shape\n",
    "\n",
    "        # Note: find the index where the previous run was interrupted\n",
    "        while filled_len < results.shape[0] and not results.iloc[filled_len, :].isnull().all():\n",
    "            filled_len += 1\n",
    "\n",
    "    except (AssertionError, FileNotFoundError):\n",
    "        results = pd.DataFrame(index=index, columns=mtf_names)\n",
    "    \n",
    "    return results, filled_len\n",
    "\n",
    "\n",
    "results_train, start_ind_train = recover_data(filepath=filename_train,\n",
    "                                              index=data_train.index,\n",
    "                                              def_shape=(data_train.shape[0], len(mtf_names)))\n",
    "\n",
    "results_test, start_ind_test = recover_data(filepath=filename_test,\n",
    "                                            index=data_test.index,\n",
    "                                            def_shape=(data_test.shape[0], len(mtf_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start index: 736\n",
      "Test start index: 184\n"
     ]
    }
   ],
   "source": [
    "assert results_train.shape == (data_train.shape[0], len(mtf_names))\n",
    "assert results_test.shape == (data_test.shape[0], len(mtf_names))\n",
    "\n",
    "print(\"Train start index:\", start_ind_train)\n",
    "print(\"Test start index:\", start_ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate meta-features per dataset: 1407\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of candidate meta-features per dataset:\", len(mtf_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metafeatures(data: pd.DataFrame, results: pd.DataFrame, start_ind: int, output_file: str) -> None:\n",
    "    print(f\"Starting extraction from index {start_ind}...\")\n",
    "    for i, (cls, _, vals) in enumerate(data.iloc[start_ind:, :].values, start_ind):\n",
    "        ts = np.asarray(vals.split(\",\")[-size_threshold:], dtype=float)\n",
    "\n",
    "        embed_lag = tspymfe._embed.embed_lag(ts=ts, max_nlags=16)\n",
    "\n",
    "        embed_dim = max(2, tspymfe._embed.ft_emb_dim_cao(ts=ts,\n",
    "                                                         lag=embed_lag,\n",
    "                                                         dims=16,\n",
    "                                                         tol_threshold=0.2))\n",
    "\n",
    "        ts_embed = tspymfe._embed.embed_ts(ts=ts,\n",
    "                                           dim=embed_dim,\n",
    "                                           lag=embed_lag)\n",
    "        \n",
    "        extractor.fit(ts_embed, suppress_warnings=True)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            res = extractor.extract(suppress_warnings=True)\n",
    "        \n",
    "        results.iloc[i, :] = res[1]\n",
    "\n",
    "        if i % to_csv_it_num == 0:\n",
    "            results.to_csv(output_file)\n",
    "            print(f\"Saved results at index {i} in file {output_file}.\")\n",
    "    \n",
    "    results.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction from index 736...\n",
      "Starting extraction from index 184...\n"
     ]
    }
   ],
   "source": [
    "extract_metafeatures(data=data_train,\n",
    "                     results=results_train,\n",
    "                     start_ind=start_ind_train,\n",
    "                     output_file=filename_train)\n",
    "\n",
    "extract_metafeatures(data=data_test,\n",
    "                     results=results_test,\n",
    "                     start_ind=start_ind_test,\n",
    "                     output_file=filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: analysing the NaN count.\n",
    "nan_count = results_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of meta-features</th>\n",
       "      <th>Proportion of meta-features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missing values count</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310 (missing on 42.12% of all train time-series)</th>\n",
       "      <td>112</td>\n",
       "      <td>0.079602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584 (missing on 79.35% of all train time-series)</th>\n",
       "      <td>8</td>\n",
       "      <td>0.005686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736 (missing on 100.00% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Number of meta-features  \\\n",
       "Missing values count                                                         \n",
       "310 (missing on 42.12% of all train time-series)                       112   \n",
       "584 (missing on 79.35% of all train time-series)                         8   \n",
       "736 (missing on 100.00% of all train time-series)                        1   \n",
       "\n",
       "                                                   Proportion of meta-features  \n",
       "Missing values count                                                            \n",
       "310 (missing on 42.12% of all train time-series)                      0.079602  \n",
       "584 (missing on 79.35% of all train time-series)                      0.005686  \n",
       "736 (missing on 100.00% of all train time-series)                     0.000711  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_nan_count = nan_count.iloc[nan_count.to_numpy().nonzero()].value_counts()\n",
    "pd_nan_count = pd.concat([pd_nan_count, pd_nan_count / results_train.shape[1]], axis=1)\n",
    "pd_nan_count = pd_nan_count.rename(columns={0: \"Number of meta-features\", 1: \"Proportion of meta-features\"})\n",
    "pd_nan_count.index =  map(\"{} (missing on {:.2f}% of all train time-series)\".format, pd_nan_count.index, 100. * pd_nan_count.index / results_train.shape[0])\n",
    "pd_nan_count.index.name = \"Missing values count\"\n",
    "pd_nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['num_to_cat'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Note: suspicious meta-feature with all missing value. Which is it?\n",
    "ind = (nan_count == data_train.shape[0]).to_numpy().nonzero()\n",
    "print(results_train.columns[ind])\n",
    "\n",
    "# Note afterwards: the result (\"num_to_cat\") seems reasonable, since no\n",
    "# time-series should have categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after dropping NaN column: (736, 1286)\n",
      "Dropped 121 of 1407 meta-features (8.60% from the total).\n"
     ]
    }
   ],
   "source": [
    "results_train.dropna(axis=1, inplace=True)\n",
    "print(\"Train shape after dropping NaN column:\", results_train.shape)\n",
    "print(f\"Dropped {len(mtf_names) - results_train.shape[1]} of {len(mtf_names)} meta-features \"\n",
    "      f\"({100 * (1 - results_train.shape[1] / len(mtf_names)):.2f}% from the total).\")\n",
    "results_test = results_test.loc[:, results_train.columns]\n",
    "\n",
    "# Note: sanity check if the columns where dropped correctly\n",
    "assert np.all(results_train.columns == results_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pipeline: sklearn.pipeline.Pipeline,\n",
    "                 X_train: np.ndarray,\n",
    "                 X_test: np.ndarray,\n",
    "                 y_train: np.ndarray,\n",
    "                 y_test:np.ndarray) -> float:\n",
    "    pipeline.fit(results_train)\n",
    "    \n",
    "    X_subset_train = pipeline.transform(X_train)\n",
    "    X_subset_test = pipeline.transform(X_test)\n",
    "    \n",
    "    assert X_subset_train.shape[1] == X_subset_test.shape[1]\n",
    "    \n",
    "    # Note: sanity check if train project is zero-centered\n",
    "    assert np.allclose(X_subset_train.mean(axis=0), 0.0)\n",
    "\n",
    "    print(\"Train shape after PCA:\", X_subset_train.shape)\n",
    "    print(\"Test shape after PCA :\", X_subset_test.shape)\n",
    "    print(f\"Total of {X_subset_train.shape[1]} of {X_train.shape[1]} \"\n",
    "          \"dimensions kept for 95% variance explained \"\n",
    "          f\"(reduction of {100. * (1 - X_subset_train.shape[1] / X_train.shape[1]):.2f}%).\")\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestClassifier(random_state=16)\n",
    "    rf.fit(X=X_subset_train, y=y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_subset_test)\n",
    "\n",
    "    # Note: since the test set is balanced, we can use the traditional accuracy\n",
    "    test_acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after PCA: (736, 105)\n",
      "Test shape after PCA : (184, 105)\n",
      "Total of 105 of 1286 dimensions kept for 95% variance explained (reduction of 91.84%).\n",
      "Expected accuracy by random guessing: 0.0217\n",
      "Test accuracy (pipeline A - StandardScaler): 0.4728\n"
     ]
    }
   ],
   "source": [
    "pipeline_a = sklearn.pipeline.Pipeline((\n",
    "    (\"zscore\", sklearn.preprocessing.StandardScaler()),\n",
    "    (\"pca\", sklearn.decomposition.PCA(n_components=0.95, random_state=16))\n",
    "))\n",
    "\n",
    "test_acc_a = get_accuracy(pipeline=pipeline_a,\n",
    "                          X_train=results_train.values,\n",
    "                          X_test=results_test.values,\n",
    "                          y_train=data_train.category.values,\n",
    "                          y_test=data_test.category.values)\n",
    "\n",
    "# This is equivalent of guessing only the majority class, which can be any class\n",
    "# in this case since the dataset is perfectly balanced\n",
    "print(f\"Expected accuracy by random guessing: {1 / data_test.category.unique().size:.4f}\")\n",
    "print(f\"Test accuracy (pipeline A - StandardScaler): {test_acc_a:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after PCA: (736, 63)\n",
      "Test shape after PCA : (184, 63)\n",
      "Total of 63 of 1286 dimensions kept for 95% variance explained (reduction of 95.10%).\n",
      "Expected accuracy by random guessing: 0.0217\n",
      "Test accuracy (pipeline B - RobustSigmoid) : 0.5543\n"
     ]
    }
   ],
   "source": [
    "pipeline_b = sklearn.pipeline.Pipeline((\n",
    "    (\"robsigmoid\", robust_sigmoid.RobustSigmoid()),\n",
    "    (\"pca\", sklearn.decomposition.PCA(n_components=0.95, random_state=16))\n",
    "))\n",
    "\n",
    "test_acc_b = get_accuracy(pipeline=pipeline_b,\n",
    "                          X_train=results_train.values,\n",
    "                          X_test=results_test.values,\n",
    "                          y_train=data_train.category.values,\n",
    "                          y_test=data_test.category.values)\n",
    "\n",
    "# This is equivalent of guessing only the majority class, which can be any class\n",
    "# in this case since the dataset is perfectly balanced\n",
    "print(f\"Expected accuracy by random guessing: {1 / data_test.category.unique().size:.4f}\")\n",
    "print(f\"Test accuracy (pipeline B - RobustSigmoid) : {test_acc_b:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
