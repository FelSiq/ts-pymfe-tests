{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompEngine dataset analysis\n",
    "## Analysis #4: Get TS-Pymfe meta-features accuracy\n",
    "\n",
    "**Project URL:** https://www.comp-engine.org/\n",
    "\n",
    "**Get data in:** https://www.comp-engine.org/#!browse\n",
    "\n",
    "**Date:** May 31 2020\n",
    "\n",
    "### Objectives:\n",
    "1. Extract the meta-features using the ts-pymfe from train and test data\n",
    "2. Drop metafeatures with NaN.\n",
    "3. Apply PCA in the train meta-dataset.\n",
    "4. Use a simple machine learning model to predict the test set.\n",
    "\n",
    "### Results (please check the analysis date):\n",
    "1. All metafeatures from all methods combined with all summary functions in pymfe were extracted from both train and test data. This totalizes 5165 candidate meta-features.\n",
    "2. Meta-features with infinities or NaN where dropped. Those are mostly related to seasonality, since it seems that only roughly 30% of the used time-series present seasonal behaviour. Dropped 1352 of 5165 meta-features (26.18% from the total), and therefore 3813 meta-feature remains.\n",
    "3. The next step is to apply PCA retaining 95% of variance explained by the original meta-features. Before applying PCA we need to choose a normalization strategy. Two methods were considered:\n",
    "    1. (Pipeline A) Standard Scaler (traditional standardization): 251 of 3813 dimensions were kept. This corresponds to a dimension reduction of 93.42%.\n",
    "    2. (Pipeline B) Robust Sigmoid Scaler (see reference [1]): 223 of 3813 dimensions were kept. This corresponds to a dimension reduction of 94.15%.\n",
    "4. Now it is time for some predictions. I'm using a sklearn RandomForestClassifier model with default hyper-parameters with a fixed random seed.\n",
    "    1. The expected accuracy of random guessing is 2.17%.\n",
    "    2. (Pipeline A1) It was obtained an accuracy score of 62.50%.\n",
    "    3. (Pipeline B1) It was obtained an accuracy score of 62.50%.\n",
    "    4. Even though both pipelines presented the same accuracy score, the Pipeline B is still superior since it needed less dimensions.    \n",
    "5. Repeating the last two steps, but with PCA retaining only 75% of variance explained:\n",
    "    1. (Pipeline A2) Standard Scaler: 50 of 3813 dimensions were kept. This corresponds to a dimension reduction of 98.69%.\n",
    "    2. (Pipeline B2) Robust Sigmoid Scaler: 30 of 3813 dimensions were kept. This corresponds to a dimension reduction of 99.21%.\n",
    "6. Accuracy (PCA 75%):\n",
    "    2. (Pipeline A2) It was obtained an accuracy score of 62.50%.\n",
    "    3. (Pipeline B2) It was obtained an accuracy score of 67.39%.\n",
    "\n",
    "\n",
    "## references:\n",
    "\n",
    ".. [1] Fulcher, Ben D.  and Little, Max A.  and Jones, Nick S., \"Highly comparative time-series analysis: the empirical structure of time series and their methods\" (Supplemental material #1, page 11), Journal of The Royal Society Interface, 2013, doi: 10.1098/rsif.2013.0048, https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2013.0048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "\n",
    "import robust_sigmoid\n",
    "import pymfe.tsmfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: using only groups that has at least one meta-feature that can be extracted\n",
    "# from a unsupervised dataset\n",
    "groups = \"all\"\n",
    "summary = \"all\"\n",
    "\n",
    "extractor = pymfe.tsmfe.TSMFE(features=\"all\",\n",
    "                              summary=summary,\n",
    "                              groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../2_exploring_subsample/subsample_train.csv\", header=0, index_col=\"timeseries_id\")\n",
    "data_test = pd.read_csv(\"../2_exploring_subsample/subsample_test.csv\", header=0, index_col=\"timeseries_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>inst_ind</th>\n",
       "      <th>datapoints</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeseries_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e0b36e39-3872-11e8-8680-0242ac120002</th>\n",
       "      <td>Beta noise</td>\n",
       "      <td>25254</td>\n",
       "      <td>0.73617,0.99008,0.71331,0.87094,0.75527,0.9912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81db0cf2-3883-11e8-8680-0242ac120002</th>\n",
       "      <td>Relative humidity</td>\n",
       "      <td>14878</td>\n",
       "      <td>95.5,79,86.75,8.75,62.75,98.75,79.74,44.75,92....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380eb353-387a-11e8-8680-0242ac120002</th>\n",
       "      <td>RR</td>\n",
       "      <td>6577</td>\n",
       "      <td>0.6328,0.6328,0.625,0.6328,0.625,0.625,0.6172,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f33f461c-3871-11e8-8680-0242ac120002</th>\n",
       "      <td>Tremor</td>\n",
       "      <td>27821</td>\n",
       "      <td>-0.6,1.5,1.5,0.1,0.9,0.6,0.3,-0.2,0.7,1,0.1,1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7bcad309-3874-11e8-8680-0242ac120002</th>\n",
       "      <td>Noisy sinusoids</td>\n",
       "      <td>14226</td>\n",
       "      <td>0.38553,0.2014,1.8705,0.47883,0.33958,0.009558...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               category  inst_ind  \\\n",
       "timeseries_id                                                       \n",
       "e0b36e39-3872-11e8-8680-0242ac120002         Beta noise     25254   \n",
       "81db0cf2-3883-11e8-8680-0242ac120002  Relative humidity     14878   \n",
       "380eb353-387a-11e8-8680-0242ac120002                 RR      6577   \n",
       "f33f461c-3871-11e8-8680-0242ac120002             Tremor     27821   \n",
       "7bcad309-3874-11e8-8680-0242ac120002    Noisy sinusoids     14226   \n",
       "\n",
       "                                                                             datapoints  \n",
       "timeseries_id                                                                            \n",
       "e0b36e39-3872-11e8-8680-0242ac120002  0.73617,0.99008,0.71331,0.87094,0.75527,0.9912...  \n",
       "81db0cf2-3883-11e8-8680-0242ac120002  95.5,79,86.75,8.75,62.75,98.75,79.74,44.75,92....  \n",
       "380eb353-387a-11e8-8680-0242ac120002  0.6328,0.6328,0.625,0.6328,0.625,0.625,0.6172,...  \n",
       "f33f461c-3871-11e8-8680-0242ac120002  -0.6,1.5,1.5,0.1,0.9,0.6,0.3,-0.2,0.7,1,0.1,1....  \n",
       "7bcad309-3874-11e8-8680-0242ac120002  0.38553,0.2014,1.8705,0.47883,0.33958,0.009558...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert data_train.shape[0] > data_test.shape[0]\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Warning: divide by zero encountered in double_scalars\n",
      " * Warning: invalid value encountered in multiply\n",
      " * Warning: Covariance of the parameters could not be estimated\n",
      " * Warning: Mean of empty slice.\n",
      " * Warning: invalid value encountered in double_scalars\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: Mean of empty slice.\n",
      " * Warning: invalid value encountered in double_scalars\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: Mean of empty slice.\n",
      " * Warning: invalid value encountered in double_scalars\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: Mean of empty slice.\n",
      " * Warning: invalid value encountered in double_scalars\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: Mean of empty slice\n",
      " * Warning: invalid value encountered in arccos\n"
     ]
    }
   ],
   "source": [
    "# Note: using at most the last 1024 observations of each time-series\n",
    "size_threshold = 1024\n",
    "\n",
    "# Number of iterations until to save results to .csv\n",
    "to_csv_it_num = 8\n",
    "\n",
    "# Note: using dummy data to get the metafeature names\n",
    "mtf_names = extractor.fit(np.random.randn(128),\n",
    "                          ts_period=1,\n",
    "                          suppress_warnings=True).extract(suppress_warnings=True)[0]\n",
    "\n",
    "# Note: filepath to store the results\n",
    "filename_train = \"metafeatures_tspymfe_train.csv\"\n",
    "filename_test = \"metafeatures_tspymfe_test.csv\"\n",
    "\n",
    "def recover_data(filepath: str,\n",
    "                 index: typing.Collection[str],\n",
    "                 def_shape: typing.Tuple[int, int]) -> typing.Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"Recover data from the previous experiment run.\"\"\"\n",
    "    filled_len = 0\n",
    "    \n",
    "    try:\n",
    "        results = pd.read_csv(filepath, index_col=0)\n",
    "        \n",
    "        assert results.shape == def_shape\n",
    "\n",
    "        # Note: find the index where the previous run was interrupted\n",
    "        while filled_len < results.shape[0] and not results.iloc[filled_len, :].isnull().all():\n",
    "            filled_len += 1\n",
    "\n",
    "    except (AssertionError, FileNotFoundError):\n",
    "        results = pd.DataFrame(index=index, columns=mtf_names)\n",
    "    \n",
    "    return results, filled_len\n",
    "\n",
    "\n",
    "results_train, start_ind_train = recover_data(filepath=filename_train,\n",
    "                                              index=data_train.index,\n",
    "                                              def_shape=(data_train.shape[0], len(mtf_names)))\n",
    "\n",
    "results_test, start_ind_test = recover_data(filepath=filename_test,\n",
    "                                            index=data_test.index,\n",
    "                                            def_shape=(data_test.shape[0], len(mtf_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start index: 736\n",
      "Test start index: 184\n"
     ]
    }
   ],
   "source": [
    "assert results_train.shape == (data_train.shape[0], len(mtf_names))\n",
    "assert results_test.shape == (data_test.shape[0], len(mtf_names))\n",
    "\n",
    "print(\"Train start index:\", start_ind_train)\n",
    "print(\"Test start index:\", start_ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate meta-features per dataset: 5165\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of candidate meta-features per dataset:\", len(mtf_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metafeatures(data: pd.DataFrame, results: pd.DataFrame, start_ind: int, output_file: str) -> None:\n",
    "    print(f\"Starting extraction from index {start_ind}...\")\n",
    "    for i, (cls, _, vals) in enumerate(data.iloc[start_ind:, :].values, start_ind):\n",
    "        ts = np.asarray(vals.split(\",\")[-size_threshold:], dtype=float)\n",
    "        extractor.fit(ts, verbose=1, suppress_warnings=True)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            res = extractor.extract(verbose=1, suppress_warnings=True)\n",
    "        \n",
    "        results.iloc[i, :] = res[1]\n",
    "\n",
    "        if i % to_csv_it_num == 0:\n",
    "            results.to_csv(output_file)\n",
    "            print(f\"Saved results at index {i} in file {output_file}.\")\n",
    "    \n",
    "    results.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction from index 736...\n",
      "Starting extraction from index 184...\n"
     ]
    }
   ],
   "source": [
    "extract_metafeatures(data=data_train,\n",
    "                     results=results_train,\n",
    "                     start_ind=start_ind_train,\n",
    "                     output_file=filename_train)\n",
    "\n",
    "extract_metafeatures(data=data_test,\n",
    "                     results=results_test,\n",
    "                     start_ind=start_ind_test,\n",
    "                     output_file=filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: analysing the NaN count.\n",
    "results_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "nan_count = results_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of meta-features</th>\n",
       "      <th>Proportion of meta-features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missing values count</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 (missing on 0.14% of all train time-series)</th>\n",
       "      <td>426</td>\n",
       "      <td>0.082478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254 (missing on 34.51% of all train time-series)</th>\n",
       "      <td>140</td>\n",
       "      <td>0.027106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521 (missing on 70.79% of all train time-series)</th>\n",
       "      <td>89</td>\n",
       "      <td>0.017231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 (missing on 0.41% of all train time-series)</th>\n",
       "      <td>84</td>\n",
       "      <td>0.016263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 (missing on 0.95% of all train time-series)</th>\n",
       "      <td>84</td>\n",
       "      <td>0.016263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 (missing on 1.36% of all train time-series)</th>\n",
       "      <td>84</td>\n",
       "      <td>0.016263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105 (missing on 14.27% of all train time-series)</th>\n",
       "      <td>57</td>\n",
       "      <td>0.011036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652 (missing on 88.59% of all train time-series)</th>\n",
       "      <td>56</td>\n",
       "      <td>0.010842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 (missing on 0.27% of all train time-series)</th>\n",
       "      <td>56</td>\n",
       "      <td>0.010842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570 (missing on 77.45% of all train time-series)</th>\n",
       "      <td>52</td>\n",
       "      <td>0.010068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22 (missing on 2.99% of all train time-series)</th>\n",
       "      <td>28</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179 (missing on 24.32% of all train time-series)</th>\n",
       "      <td>28</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631 (missing on 85.73% of all train time-series)</th>\n",
       "      <td>28</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17 (missing on 2.31% of all train time-series)</th>\n",
       "      <td>28</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103 (missing on 13.99% of all train time-series)</th>\n",
       "      <td>26</td>\n",
       "      <td>0.005034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403 (missing on 54.76% of all train time-series)</th>\n",
       "      <td>21</td>\n",
       "      <td>0.004066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13 (missing on 1.77% of all train time-series)</th>\n",
       "      <td>19</td>\n",
       "      <td>0.003679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97 (missing on 13.18% of all train time-series)</th>\n",
       "      <td>8</td>\n",
       "      <td>0.001549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401 (missing on 54.48% of all train time-series)</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 (missing on 1.49% of all train time-series)</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603 (missing on 81.93% of all train time-series)</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8 (missing on 1.09% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15 (missing on 2.04% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37 (missing on 5.03% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96 (missing on 13.04% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106 (missing on 14.40% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127 (missing on 17.26% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128 (missing on 17.39% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148 (missing on 20.11% of all train time-series)</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16 (missing on 2.17% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110 (missing on 14.95% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12 (missing on 1.63% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560 (missing on 76.09% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9 (missing on 1.22% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406 (missing on 55.16% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271 (missing on 36.82% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405 (missing on 55.03% of all train time-series)</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Number of meta-features  \\\n",
       "Missing values count                                                        \n",
       "1 (missing on 0.14% of all train time-series)                         426   \n",
       "254 (missing on 34.51% of all train time-series)                      140   \n",
       "521 (missing on 70.79% of all train time-series)                       89   \n",
       "3 (missing on 0.41% of all train time-series)                          84   \n",
       "7 (missing on 0.95% of all train time-series)                          84   \n",
       "10 (missing on 1.36% of all train time-series)                         84   \n",
       "105 (missing on 14.27% of all train time-series)                       57   \n",
       "652 (missing on 88.59% of all train time-series)                       56   \n",
       "2 (missing on 0.27% of all train time-series)                          56   \n",
       "570 (missing on 77.45% of all train time-series)                       52   \n",
       "22 (missing on 2.99% of all train time-series)                         28   \n",
       "179 (missing on 24.32% of all train time-series)                       28   \n",
       "631 (missing on 85.73% of all train time-series)                       28   \n",
       "17 (missing on 2.31% of all train time-series)                         28   \n",
       "103 (missing on 13.99% of all train time-series)                       26   \n",
       "403 (missing on 54.76% of all train time-series)                       21   \n",
       "13 (missing on 1.77% of all train time-series)                         19   \n",
       "97 (missing on 13.18% of all train time-series)                         8   \n",
       "401 (missing on 54.48% of all train time-series)                        5   \n",
       "11 (missing on 1.49% of all train time-series)                          5   \n",
       "603 (missing on 81.93% of all train time-series)                        4   \n",
       "8 (missing on 1.09% of all train time-series)                           2   \n",
       "15 (missing on 2.04% of all train time-series)                          2   \n",
       "37 (missing on 5.03% of all train time-series)                          2   \n",
       "96 (missing on 13.04% of all train time-series)                         2   \n",
       "106 (missing on 14.40% of all train time-series)                        2   \n",
       "127 (missing on 17.26% of all train time-series)                        2   \n",
       "128 (missing on 17.39% of all train time-series)                        2   \n",
       "148 (missing on 20.11% of all train time-series)                        2   \n",
       "16 (missing on 2.17% of all train time-series)                          1   \n",
       "110 (missing on 14.95% of all train time-series)                        1   \n",
       "12 (missing on 1.63% of all train time-series)                          1   \n",
       "560 (missing on 76.09% of all train time-series)                        1   \n",
       "9 (missing on 1.22% of all train time-series)                           1   \n",
       "406 (missing on 55.16% of all train time-series)                        1   \n",
       "271 (missing on 36.82% of all train time-series)                        1   \n",
       "405 (missing on 55.03% of all train time-series)                        1   \n",
       "\n",
       "                                                  Proportion of meta-features  \n",
       "Missing values count                                                           \n",
       "1 (missing on 0.14% of all train time-series)                        0.082478  \n",
       "254 (missing on 34.51% of all train time-series)                     0.027106  \n",
       "521 (missing on 70.79% of all train time-series)                     0.017231  \n",
       "3 (missing on 0.41% of all train time-series)                        0.016263  \n",
       "7 (missing on 0.95% of all train time-series)                        0.016263  \n",
       "10 (missing on 1.36% of all train time-series)                       0.016263  \n",
       "105 (missing on 14.27% of all train time-series)                     0.011036  \n",
       "652 (missing on 88.59% of all train time-series)                     0.010842  \n",
       "2 (missing on 0.27% of all train time-series)                        0.010842  \n",
       "570 (missing on 77.45% of all train time-series)                     0.010068  \n",
       "22 (missing on 2.99% of all train time-series)                       0.005421  \n",
       "179 (missing on 24.32% of all train time-series)                     0.005421  \n",
       "631 (missing on 85.73% of all train time-series)                     0.005421  \n",
       "17 (missing on 2.31% of all train time-series)                       0.005421  \n",
       "103 (missing on 13.99% of all train time-series)                     0.005034  \n",
       "403 (missing on 54.76% of all train time-series)                     0.004066  \n",
       "13 (missing on 1.77% of all train time-series)                       0.003679  \n",
       "97 (missing on 13.18% of all train time-series)                      0.001549  \n",
       "401 (missing on 54.48% of all train time-series)                     0.000968  \n",
       "11 (missing on 1.49% of all train time-series)                       0.000968  \n",
       "603 (missing on 81.93% of all train time-series)                     0.000774  \n",
       "8 (missing on 1.09% of all train time-series)                        0.000387  \n",
       "15 (missing on 2.04% of all train time-series)                       0.000387  \n",
       "37 (missing on 5.03% of all train time-series)                       0.000387  \n",
       "96 (missing on 13.04% of all train time-series)                      0.000387  \n",
       "106 (missing on 14.40% of all train time-series)                     0.000387  \n",
       "127 (missing on 17.26% of all train time-series)                     0.000387  \n",
       "128 (missing on 17.39% of all train time-series)                     0.000387  \n",
       "148 (missing on 20.11% of all train time-series)                     0.000387  \n",
       "16 (missing on 2.17% of all train time-series)                       0.000194  \n",
       "110 (missing on 14.95% of all train time-series)                     0.000194  \n",
       "12 (missing on 1.63% of all train time-series)                       0.000194  \n",
       "560 (missing on 76.09% of all train time-series)                     0.000194  \n",
       "9 (missing on 1.22% of all train time-series)                        0.000194  \n",
       "406 (missing on 55.16% of all train time-series)                     0.000194  \n",
       "271 (missing on 36.82% of all train time-series)                     0.000194  \n",
       "405 (missing on 55.03% of all train time-series)                     0.000194  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_nan_count = nan_count.iloc[nan_count.to_numpy().nonzero()].value_counts()\n",
    "pd_nan_count = pd.concat([pd_nan_count, pd_nan_count / results_train.shape[1]], axis=1)\n",
    "pd_nan_count = pd_nan_count.rename(columns={0: \"Number of meta-features\", 1: \"Proportion of meta-features\"})\n",
    "pd_nan_count.index =  map(\"{} (missing on {:.2f}% of all train time-series)\".format, pd_nan_count.index, 100. * pd_nan_count.index / results_train.shape[0])\n",
    "pd_nan_count.index.name = \"Missing values count\"\n",
    "pd_nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avg_cycle_period', 'ets_level', 'ets_season', 'ets_slope', 'model_hwes_ada.histogram.0', 'model_hwes_ada.histogram.1', 'model_hwes_ada.histogram.2', 'model_hwes_ada.histogram.3', 'model_hwes_ada.histogram.4', 'model_hwes_ada.histogram.5', 'model_hwes_ada.histogram.6', 'model_hwes_ada.histogram.7', 'model_hwes_ada.histogram.8', 'model_hwes_ada.histogram.9', 'model_hwes_ada.iq_range', 'model_hwes_ada.kurtosis', 'model_hwes_ada.max', 'model_hwes_ada.mean', 'model_hwes_ada.median', 'model_hwes_ada.min', 'model_hwes_ada.nanhistogram.0', 'model_hwes_ada.nanhistogram.1', 'model_hwes_ada.nanhistogram.2', 'model_hwes_ada.nanhistogram.3', 'model_hwes_ada.nanhistogram.4', 'model_hwes_ada.nanhistogram.5', 'model_hwes_ada.nanhistogram.6', 'model_hwes_ada.nanhistogram.7', 'model_hwes_ada.nanhistogram.8', 'model_hwes_ada.nanhistogram.9', 'model_hwes_ada.naniq_range', 'model_hwes_ada.nankurtosis', 'model_hwes_ada.nanmax', 'model_hwes_ada.nanmean', 'model_hwes_ada.nanmedian', 'model_hwes_ada.nanmin', 'model_hwes_ada.nanpnorm', 'model_hwes_ada.nanpowersum', 'model_hwes_ada.nanquantiles.0', 'model_hwes_ada.nanquantiles.1', 'model_hwes_ada.nanquantiles.2', 'model_hwes_ada.nanquantiles.3', 'model_hwes_ada.nanquantiles.4', 'model_hwes_ada.nanrange', 'model_hwes_ada.nansd', 'model_hwes_ada.nanskewness', 'model_hwes_ada.nansum', 'model_hwes_ada.nanvar', 'model_hwes_ada.pnorm', 'model_hwes_ada.powersum', 'model_hwes_ada.quantiles.0', 'model_hwes_ada.quantiles.1', 'model_hwes_ada.quantiles.2', 'model_hwes_ada.quantiles.3', 'model_hwes_ada.quantiles.4', 'model_hwes_ada.range', 'model_hwes_ada.sd', 'model_hwes_ada.skewness', 'model_hwes_ada.sum', 'model_hwes_ada.var', 'model_hwes_adm.histogram.0', 'model_hwes_adm.histogram.1', 'model_hwes_adm.histogram.2', 'model_hwes_adm.histogram.3', 'model_hwes_adm.histogram.4', 'model_hwes_adm.histogram.5', 'model_hwes_adm.histogram.6', 'model_hwes_adm.histogram.7', 'model_hwes_adm.histogram.8', 'model_hwes_adm.histogram.9', 'model_hwes_adm.iq_range', 'model_hwes_adm.kurtosis', 'model_hwes_adm.max', 'model_hwes_adm.mean', 'model_hwes_adm.median', 'model_hwes_adm.min', 'model_hwes_adm.nanhistogram.0', 'model_hwes_adm.nanhistogram.1', 'model_hwes_adm.nanhistogram.2', 'model_hwes_adm.nanhistogram.3', 'model_hwes_adm.nanhistogram.4', 'model_hwes_adm.nanhistogram.5', 'model_hwes_adm.nanhistogram.6', 'model_hwes_adm.nanhistogram.7', 'model_hwes_adm.nanhistogram.8', 'model_hwes_adm.nanhistogram.9', 'model_hwes_adm.naniq_range', 'model_hwes_adm.nankurtosis', 'model_hwes_adm.nanmax', 'model_hwes_adm.nanmean', 'model_hwes_adm.nanmedian', 'model_hwes_adm.nanmin', 'model_hwes_adm.nanpnorm', 'model_hwes_adm.nanpowersum', 'model_hwes_adm.nanquantiles.0', 'model_hwes_adm.nanquantiles.1', 'model_hwes_adm.nanquantiles.2', 'model_hwes_adm.nanquantiles.3', 'model_hwes_adm.nanquantiles.4', 'model_hwes_adm.nanrange', 'model_hwes_adm.nansd', 'model_hwes_adm.nanskewness', 'model_hwes_adm.nansum', 'model_hwes_adm.nanvar', 'model_hwes_adm.pnorm', 'model_hwes_adm.powersum', 'model_hwes_adm.quantiles.0', 'model_hwes_adm.quantiles.1', 'model_hwes_adm.quantiles.2', 'model_hwes_adm.quantiles.3', 'model_hwes_adm.quantiles.4', 'model_hwes_adm.range', 'model_hwes_adm.sd', 'model_hwes_adm.skewness', 'model_hwes_adm.sum', 'model_hwes_adm.var', 'model_linear_seasonal.histogram.0', 'model_linear_seasonal.histogram.1', 'model_linear_seasonal.histogram.2', 'model_linear_seasonal.histogram.3', 'model_linear_seasonal.histogram.4', 'model_linear_seasonal.histogram.5', 'model_linear_seasonal.histogram.6', 'model_linear_seasonal.histogram.7', 'model_linear_seasonal.histogram.8', 'model_linear_seasonal.histogram.9', 'model_linear_seasonal.iq_range', 'model_linear_seasonal.kurtosis', 'model_linear_seasonal.max', 'model_linear_seasonal.mean', 'model_linear_seasonal.median', 'model_linear_seasonal.min', 'model_linear_seasonal.nanhistogram.0', 'model_linear_seasonal.nanhistogram.1', 'model_linear_seasonal.nanhistogram.2', 'model_linear_seasonal.nanhistogram.3', 'model_linear_seasonal.nanhistogram.4', 'model_linear_seasonal.nanhistogram.5', 'model_linear_seasonal.nanhistogram.6', 'model_linear_seasonal.nanhistogram.7', 'model_linear_seasonal.nanhistogram.8', 'model_linear_seasonal.nanhistogram.9', 'model_linear_seasonal.naniq_range', 'model_linear_seasonal.nankurtosis', 'model_linear_seasonal.nanmax', 'model_linear_seasonal.nanmean', 'model_linear_seasonal.nanmedian', 'model_linear_seasonal.nanmin', 'model_linear_seasonal.nanpnorm', 'model_linear_seasonal.nanpowersum', 'model_linear_seasonal.nanquantiles.0', 'model_linear_seasonal.nanquantiles.1', 'model_linear_seasonal.nanquantiles.2', 'model_linear_seasonal.nanquantiles.3', 'model_linear_seasonal.nanquantiles.4', 'model_linear_seasonal.nanrange', 'model_linear_seasonal.nansd', 'model_linear_seasonal.nanskewness', 'model_linear_seasonal.nansum', 'model_linear_seasonal.nanvar', 'model_linear_seasonal.pnorm', 'model_linear_seasonal.powersum', 'model_linear_seasonal.quantiles.0', 'model_linear_seasonal.quantiles.1', 'model_linear_seasonal.quantiles.2', 'model_linear_seasonal.quantiles.3', 'model_linear_seasonal.quantiles.4', 'model_linear_seasonal.range', 'model_linear_seasonal.sd', 'model_linear_seasonal.skewness', 'model_linear_seasonal.sum', 'model_linear_seasonal.var', 'model_naive_seasonal.histogram.0', 'model_naive_seasonal.histogram.1', 'model_naive_seasonal.histogram.2', 'model_naive_seasonal.histogram.3', 'model_naive_seasonal.histogram.4', 'model_naive_seasonal.histogram.5', 'model_naive_seasonal.histogram.6', 'model_naive_seasonal.histogram.7', 'model_naive_seasonal.histogram.8', 'model_naive_seasonal.histogram.9', 'model_naive_seasonal.iq_range', 'model_naive_seasonal.kurtosis', 'model_naive_seasonal.max', 'model_naive_seasonal.mean', 'model_naive_seasonal.median', 'model_naive_seasonal.min', 'model_naive_seasonal.nanhistogram.0', 'model_naive_seasonal.nanhistogram.1', 'model_naive_seasonal.nanhistogram.2', 'model_naive_seasonal.nanhistogram.3', 'model_naive_seasonal.nanhistogram.4', 'model_naive_seasonal.nanhistogram.5', 'model_naive_seasonal.nanhistogram.6', 'model_naive_seasonal.nanhistogram.7', 'model_naive_seasonal.nanhistogram.8', 'model_naive_seasonal.nanhistogram.9', 'model_naive_seasonal.naniq_range', 'model_naive_seasonal.nankurtosis', 'model_naive_seasonal.nanmax', 'model_naive_seasonal.nanmean', 'model_naive_seasonal.nanmedian', 'model_naive_seasonal.nanmin', 'model_naive_seasonal.nanpnorm', 'model_naive_seasonal.nanpowersum', 'model_naive_seasonal.nanquantiles.0', 'model_naive_seasonal.nanquantiles.1', 'model_naive_seasonal.nanquantiles.2', 'model_naive_seasonal.nanquantiles.3', 'model_naive_seasonal.nanquantiles.4', 'model_naive_seasonal.nanrange', 'model_naive_seasonal.nansd', 'model_naive_seasonal.nanskewness', 'model_naive_seasonal.nansum', 'model_naive_seasonal.nanvar', 'model_naive_seasonal.pnorm', 'model_naive_seasonal.powersum', 'model_naive_seasonal.quantiles.0', 'model_naive_seasonal.quantiles.1', 'model_naive_seasonal.quantiles.2', 'model_naive_seasonal.quantiles.3', 'model_naive_seasonal.quantiles.4', 'model_naive_seasonal.range', 'model_naive_seasonal.sd', 'model_naive_seasonal.skewness', 'model_naive_seasonal.sum', 'model_naive_seasonal.var', 'peak_frac', 'trough_frac']\n"
     ]
    }
   ],
   "source": [
    "# Note: some meta-features with high rates of missing values. Which are those?\n",
    "ind = (nan_count >= 0.7 * data_train.shape[0]).to_numpy().nonzero()\n",
    "print(list(results_train.columns[ind]))\n",
    "\n",
    "# Post note: all meta-features with high rates of missing values seems to be\n",
    "# related to seasonality or cyclicity. This says something about the used subset:\n",
    "# roughly 70% of the time-series are not seasonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after dropping NaN column: (736, 3813)\n",
      "Dropped 1352 of 5165 meta-features (26.18% from the total).\n"
     ]
    }
   ],
   "source": [
    "results_train.dropna(axis=1, inplace=True)\n",
    "print(\"Train shape after dropping NaN column:\", results_train.shape)\n",
    "print(f\"Dropped {len(mtf_names) - results_train.shape[1]} of {len(mtf_names)} meta-features \"\n",
    "      f\"({100 * (1 - results_train.shape[1] / len(mtf_names)):.2f}% from the total).\")\n",
    "results_test = results_test.loc[:, results_train.columns]\n",
    "\n",
    "# Note: sanity check if the columns where dropped correctly\n",
    "assert np.all(results_train.columns == results_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pipeline: sklearn.pipeline.Pipeline,\n",
    "                 X_train: np.ndarray,\n",
    "                 X_test: np.ndarray,\n",
    "                 y_train: np.ndarray,\n",
    "                 y_test:np.ndarray) -> float:\n",
    "    pipeline.fit(results_train)\n",
    "    \n",
    "    X_subset_train = pipeline.transform(X_train)\n",
    "    X_subset_test = pipeline.transform(X_test)\n",
    "    \n",
    "    assert X_subset_train.shape[1] == X_subset_test.shape[1]\n",
    "    \n",
    "    # Note: sanity check if train project is zero-centered\n",
    "    assert np.allclose(X_subset_train.mean(axis=0), 0.0)\n",
    "\n",
    "    print(\"Train shape after PCA:\", X_subset_train.shape)\n",
    "    print(\"Test shape after PCA :\", X_subset_test.shape)\n",
    "    print(f\"Total of {X_subset_train.shape[1]} of {X_train.shape[1]} \"\n",
    "          f\"dimensions kept for {100. * var_explained:.2f}% variance explained \"\n",
    "          f\"(reduction of {100. * (1 - X_subset_train.shape[1] / X_train.shape[1]):.2f}%).\")\n",
    "    \n",
    "    rf = sklearn.ensemble.RandomForestClassifier(random_state=16)\n",
    "    rf.fit(X=X_subset_train, y=y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_subset_test)\n",
    "\n",
    "    # Note: since the test set is balanced, we can use the traditional accuracy\n",
    "    test_acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after PCA: (736, 251)\n",
      "Test shape after PCA : (184, 251)\n",
      "Total of 251 of 3813 dimensions kept for 95.00% variance explained (reduction of 93.42%).\n",
      "Expected accuracy by random guessing: 0.0217\n",
      "Test accuracy (pipeline A1 - StandardScaler (VE 95%)): 0.6250\n"
     ]
    }
   ],
   "source": [
    "var_explained = 0.95\n",
    "\n",
    "pipeline_a1 = sklearn.pipeline.Pipeline((\n",
    "    (\"zscore\", sklearn.preprocessing.StandardScaler()),\n",
    "    (\"pca95\", sklearn.decomposition.PCA(n_components=var_explained, random_state=16))\n",
    "))\n",
    "\n",
    "test_acc_a1 = get_accuracy(pipeline=pipeline_a1,\n",
    "                           X_train=results_train.values,\n",
    "                           X_test=results_test.values,\n",
    "                           y_train=data_train.category.values,\n",
    "                           y_test=data_test.category.values)\n",
    "\n",
    "# This is equivalent of guessing only the majority class, which can be any class\n",
    "# in this case since the dataset is perfectly balanced\n",
    "print(f\"Expected accuracy by random guessing: {1 / data_test.category.unique().size:.4f}\")\n",
    "print(f\"Test accuracy (pipeline A1 - StandardScaler (VE 95%)): {test_acc_a1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after PCA: (736, 50)\n",
      "Test shape after PCA : (184, 50)\n",
      "Total of 50 of 3813 dimensions kept for 75.00% variance explained (reduction of 98.69%).\n",
      "Expected accuracy by random guessing: 0.0217\n",
      "Test accuracy (pipeline A2 - StandardScaler (VE 75%)): 0.6250\n"
     ]
    }
   ],
   "source": [
    "var_explained = 0.75\n",
    "\n",
    "pipeline_a2 = sklearn.pipeline.Pipeline((\n",
    "    (\"zscore\", sklearn.preprocessing.StandardScaler()),\n",
    "    (\"pca75\", sklearn.decomposition.PCA(n_components=var_explained, random_state=16))\n",
    "))\n",
    "\n",
    "test_acc_a2 = get_accuracy(pipeline=pipeline_a2,\n",
    "                           X_train=results_train.values,\n",
    "                           X_test=results_test.values,\n",
    "                           y_train=data_train.category.values,\n",
    "                           y_test=data_test.category.values)\n",
    "\n",
    "# This is equivalent of guessing only the majority class, which can be any class\n",
    "# in this case since the dataset is perfectly balanced\n",
    "print(f\"Expected accuracy by random guessing: {1 / data_test.category.unique().size:.4f}\")\n",
    "print(f\"Test accuracy (pipeline A2 - StandardScaler (VE 75%)): {test_acc_a2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after PCA: (736, 223)\n",
      "Test shape after PCA : (184, 223)\n",
      "Total of 223 of 3813 dimensions kept for 95.00% variance explained (reduction of 94.15%).\n",
      "Expected accuracy by random guessing: 0.0217\n",
      "Test accuracy (pipeline B1 - RobustSigmoid (VE 95%)) : 0.6250\n"
     ]
    }
   ],
   "source": [
    "var_explained = 0.95\n",
    "\n",
    "pipeline_b1 = sklearn.pipeline.Pipeline((\n",
    "    (\"robsigmoid\", robust_sigmoid.RobustSigmoid()),\n",
    "    (\"pca95\", sklearn.decomposition.PCA(n_components=var_explained, random_state=16))\n",
    "))\n",
    "\n",
    "test_acc_b1 = get_accuracy(pipeline=pipeline_b1,\n",
    "                           X_train=results_train.values,\n",
    "                           X_test=results_test.values,\n",
    "                           y_train=data_train.category.values,\n",
    "                           y_test=data_test.category.values)\n",
    "\n",
    "# This is equivalent of guessing only the majority class, which can be any class\n",
    "# in this case since the dataset is perfectly balanced\n",
    "print(f\"Expected accuracy by random guessing: {1 / data_test.category.unique().size:.4f}\")\n",
    "print(f\"Test accuracy (pipeline B1 - RobustSigmoid (VE 95%)) : {test_acc_b1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after PCA: (736, 30)\n",
      "Test shape after PCA : (184, 30)\n",
      "Total of 30 of 3813 dimensions kept for 75.00% variance explained (reduction of 99.21%).\n",
      "Expected accuracy by random guessing: 0.0217\n",
      "Test accuracy (pipeline B2 - RobustSigmoid (VE 75%)) : 0.6739\n"
     ]
    }
   ],
   "source": [
    "var_explained = 0.75\n",
    "\n",
    "pipeline_b2 = sklearn.pipeline.Pipeline((\n",
    "    (\"robsigmoid\", robust_sigmoid.RobustSigmoid()),\n",
    "    (\"pca75\", sklearn.decomposition.PCA(n_components=var_explained, random_state=16))\n",
    "))\n",
    "\n",
    "test_acc_b2 = get_accuracy(pipeline=pipeline_b2,\n",
    "                           X_train=results_train.values,\n",
    "                           X_test=results_test.values,\n",
    "                           y_train=data_train.category.values,\n",
    "                           y_test=data_test.category.values)\n",
    "\n",
    "# This is equivalent of guessing only the majority class, which can be any class\n",
    "# in this case since the dataset is perfectly balanced\n",
    "print(f\"Expected accuracy by random guessing: {1 / data_test.category.unique().size:.4f}\")\n",
    "print(f\"Test accuracy (pipeline B2 - RobustSigmoid (VE 75%)) : {test_acc_b2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
